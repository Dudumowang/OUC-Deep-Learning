# 2024年夏季《深度学习》学习报告



| 姓名和学号？         | 杨文彬，21240213247          |
| -------------------- | ---------------------------- |
| 本实验属于哪门课程？ | 中国海洋大学24夏《深度学习》 |
| 学习内容？           | 深度学习基础                 |
| 博客地址？           | XXXXXXX                      |



## **一、学习目标**

学习浅层神经⽹络、⽣物神经元到单层感知器，多层感知器，反向传播和梯度消失、神经⽹络到深度学习：逐层预训练，⾃编码器。



## 二、学习内容

1.通过视频学习浅层神经网络和神经网络到深度学习。
2.通过Colab平台进行代码练习pytorch基础练习和螺旋数据分类。

Pytorch基础练习

![](https://github.com/Dudumowang/OUC-Deep-Learning/blob/main/pic/week01/1.1.png)



## 三、程序运行结果





## 四、问题总结与体会

在本周的实验中，首先第一部分实验，练习了torch中的一些基本操作，比如向量、矩阵、张量的使用以及基本运算操作。第二部分实验，分别构建了线性分类模型和含有激活函数的非线性分类模型，并比对其效果。在两层神经网络里加入 ReLU 激活函数以后，分类的准确率得到了显著提高。

问题思考：

1、AlexNet有哪些特点？为什么可以⽐LeNet取得更好的性能？

​    AlexNet有更深的网络结构；使用了ReLu激活函数，运算更简单，且ReLU函数在不同的参数初始化方法下可以让模型更容易训练；AlexNet使用Dropout有效防止过拟合

2、激活函数有哪些作⽤？

​    引入非线性特征；控制输出范围；提供梯度信号；加速收敛；增加网络的表达能力。

3、梯度消失现象是什么？

​    在深度神经网络的训练过程中，尤其是反向传播算法中，梯度在网络的各层之间逐渐变小，最终变得非常小，导致网络的参数无法有效更新，从而训练变得缓慢或停滞。

4、神经⽹络是更宽好还是更深好？

​    选择哪个更合适取决于具体任务和目标。

​    更深的神经网络：

​        优点：学习到更多层地的特征表示，适合复杂任务，在较大规模数据集上，可以取得更好的性能。

​       缺点：训练难度大，出现梯度消失或梯度爆炸问题，需要更多的计算资源和时间。

​    更宽的神经网络：

​        优点：在较浅层次中就能提取到更多的特征，可能在较小的深度下就能提取到更好的性能。

​        缺点：网络的宽度可能增加计算资源的浪费，有冗余的神经元；容易造成过拟合尤其是在数据量比较小的时候。

5、为什么要使⽤Softmax?

​    将网络的原始输出转换为概率分布的形式；通常和交叉熵损失函数一起使用，简化损失计算；支持多类别的分类；提高模型的决策能力。

6、SGD 和 Adam 哪个更有效？

​    Adam通常更有效，尤其是在复杂的模型和大规模数据集上，可以更快地收敛，且调参相对简单。